{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:12:50.505998Z",
     "start_time": "2021-06-16T03:12:48.527459Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import local_conformal as lc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "\n",
    "| | Training|Validation|CDE-Tune|Calibrate| Test |\n",
    "|-|-|-|-|-|-| \n",
    "|Izbicki| 1n |1n|0|1n| 1n |\n",
    "|Us| .75n|.75n|.75n|.75n| 1n|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:12:50.512340Z",
     "start_time": "2021-06-16T03:12:50.508259Z"
    }
   },
   "outputs": [],
   "source": [
    "n = np.array([100,200,1000, 5000, 10000], dtype = int)\n",
    "n_sims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:12:50.532878Z",
     "start_time": "2021-06-16T03:12:50.525798Z"
    }
   },
   "outputs": [],
   "source": [
    "def k_values_select(n, n_train):\n",
    "    out = list(np.arange(2,17, dtype = int)) +\\\n",
    "            [int(n_train/100)] # Izbicki's number\n",
    "    \n",
    "    if n > 1000:\n",
    "        out += list(np.arange(17,33, dtype = int))\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:12:50.541753Z",
     "start_time": "2021-06-16T03:12:50.537097Z"
    }
   },
   "outputs": [],
   "source": [
    "def n_gaussians_select(n):\n",
    "    upper = np.max([20, np.floor(n**(1/3))])\n",
    "    out = list(np.arange(1, upper, dtype = int))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:12:50.548400Z",
     "start_time": "2021-06-16T03:12:50.544842Z"
    }
   },
   "outputs": [],
   "source": [
    "def lr_select():\n",
    "    out = list(10**(-1* np.arrange(2,7)))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:12:50.554229Z",
     "start_time": "2021-06-16T03:12:50.550138Z"
    }
   },
   "outputs": [],
   "source": [
    "def hidden_select(n):\n",
    "    n_based_value = np.floor(n**.5)\n",
    "    if (n_based_value > 10):\n",
    "        upper = int(np.min([50, n_based_value]))\n",
    "    else:\n",
    "        upper = 10\n",
    "    out = list(np.arange(5, upper, dtype = int))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:12:50.558638Z",
     "start_time": "2021-06-16T03:12:50.556081Z"
    }
   },
   "outputs": [],
   "source": [
    "def quantiles_vs_expected():\n",
    "    \"\"\"\n",
    "    for each observations cs, look at their groups in calibration mat cs\n",
    "    and calculate mean(cs_obs >= cs_cal)\n",
    "    \n",
    "    Todo: this could potentially be fit into `thresholds_per_group`?\n",
    "    \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:14:01.199121Z",
     "start_time": "2021-06-16T03:14:01.196042Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#     def full_process(n, n_sims, sigma  = 4):\n",
    "#         # get ranges of all parameters\n",
    "\n",
    "#         # or write a python script, and run this in parallel (slurm style)?\n",
    "\n",
    "#         all_info_across = pd.DataFrame(columns = [\"n_sim\",\n",
    "#                                                \"approach_type\",\n",
    "#                                                \"mdn_n_gaussian\",\n",
    "#                                                \"mdn_lr\",\n",
    "#                                                \"mdn_hidden1\",\n",
    "#                                                \"mdn_hidden2\",\n",
    "#                                                \"mdn_train_error\",\n",
    "#                                                \"mdn_val_error\",\n",
    "#                                                \"q_lr\",\n",
    "#                                                \"q_hidden1\",\n",
    "#                                                \"q_hidden2\",\n",
    "#                                                \"q_train_error\",\n",
    "#                                                \"q_val_error\",\n",
    "#                                                \"k\",\n",
    "#                                                \"x_group\",\n",
    "#                                                \"validity_truth\",\n",
    "#                                                \"validity_test\",\n",
    "#                                                \"efficiency_truth\"\n",
    "#                                                ])\n",
    "#         for sim_idx in np.arange(n_sims):\n",
    "#             data_all = lc.data_generation(n, sigma = sigma)\n",
    "#             inner_info_iz = inner_process_izbicki(data_all, ...parameters) \n",
    "#             inner_info_us = inner_process_us(data_all, ...parameters) \n",
    "\n",
    "#             inner_info[\"n_sim\"] = s_idx\n",
    "#             inner_info2 = inner_info[[\"n_sim\",\n",
    "#                                       \"approach_type\",\n",
    "#                                        \"mdn_n_gaussian\",\n",
    "#                                        \"mdn_lr\",\n",
    "#                                        \"mdn_hidden1\",\n",
    "#                                        \"mdn_hidden2\",\n",
    "#                                        \"mdn_train_error\",\n",
    "#                                        \"mdn_val_error\",\n",
    "#                                        \"q_lr\",\n",
    "#                                        \"q_hidden1\",\n",
    "#                                        \"q_hidden2\",\n",
    "#                                        \"q_train_error\",\n",
    "#                                        \"q_val_error\",\n",
    "#                                        \"k\",\n",
    "#                                        \"x_group\",\n",
    "#                                        \"validity_truth\",\n",
    "#                                        \"validity_test\",\n",
    "#                                        \"efficiency_truth\"\n",
    "#                                        ]]\n",
    "\n",
    "#             all_info_across = all_info_across.append(inner_info2)\n",
    "\n",
    "#         return all_info_across "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:14:02.256563Z",
     "start_time": "2021-06-16T03:14:02.177135Z"
    }
   },
   "outputs": [],
   "source": [
    "n = 200\n",
    "data_all = lc.data_generation(n, 4)\n",
    "\n",
    "data_izbicki_list = lc.stratified_data_splitting(data_all,\n",
    "                                                prop_vec = np.array([1,1,0,1,1])/4)\n",
    "x_train_iz,y_train_iz, g_train_iz = lc.torchify_data(data_izbicki_list[0].x, \n",
    "                                        data_izbicki_list[0].y, \n",
    "                                        data_izbicki_list[0].group_info)\n",
    "x_val_iz,y_val_iz, g_val_iz = lc.torchify_data(data_izbicki_list[1].x, \n",
    "                                        data_izbicki_list[1].y, \n",
    "                                        data_izbicki_list[1].group_info)\n",
    "x_cal_iz,y_cal_iz, g_val_iz = lc.torchify_data(data_izbicki_list[3].x, \n",
    "                                        data_izbicki_list[3].y, \n",
    "                                        data_izbicki_list[3].group_info)\n",
    "x_test_iz,y_test_iz, g_test_iz = lc.torchify_data(data_izbicki_list[4].x, \n",
    "                                        data_izbicki_list[4].y, \n",
    "                                        data_izbicki_list[4].group_info)\n",
    "\n",
    "\n",
    "\n",
    "column_order = [\n",
    "               \"approach_type\",\n",
    "               \"mdn_n_gaussian\",\n",
    "               \"mdn_lr\",\n",
    "               \"mdn_hidden1\",\n",
    "               \"mdn_hidden2\",\n",
    "               \"mdn_train_error\",\n",
    "               \"mdn_val_error\",\n",
    "               \"q_lr\",\n",
    "               \"q_hidden1\",\n",
    "               \"q_hidden2\",\n",
    "               \"q_train_error\",\n",
    "               \"q_val_error\",\n",
    "               \"k\",\n",
    "               \"x_group\",\n",
    "               \"validity_truth\",\n",
    "               \"validity_test\",\n",
    "               \"efficiency_truth\"\n",
    "               ]\n",
    "all_info_out = pd.DataFrame(columns = column_order)\n",
    "\n",
    "all_info_out\n",
    "\n",
    "\n",
    "n_guassian, lr_1, hidden1_1, hidden2_1 = 1, 1e-03, 10, 10\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:14:23.400930Z",
     "start_time": "2021-06-16T03:14:03.577003Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|#####################################| (Time:  0:00:19, Elapsed Time: 0:00:19)\n"
     ]
    }
   ],
   "source": [
    "mdn_model_iz, mdn_opt_iz, val_error_iz = lc.tune_first_nn(\n",
    "                                                 x_train = x_train_iz,\n",
    "                                                 y_train = y_train_iz,\n",
    "                                                 x_val = x_val_iz,\n",
    "                                                 y_val = y_val_iz,\n",
    "                                                 epochs = epochs,\n",
    "                                                 n_gaussians=n_guassian,\n",
    "                                                 n_hidden1 = hidden1_1,\n",
    "                                                 n_hidden2 = hidden2_1,\n",
    "                                                 lr = lr_1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:14:23.408744Z",
     "start_time": "2021-06-16T03:14:23.403841Z"
    }
   },
   "outputs": [],
   "source": [
    "y_range = np.array([-25,25]).reshape((-1,1))\n",
    "y_splits = 10000\n",
    "t_range_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:14:23.422265Z",
     "start_time": "2021-06-16T03:14:23.417157Z"
    }
   },
   "outputs": [],
   "source": [
    "mdn_model_iz.prep_for_cde(y_range, y_splits)\n",
    "model_y_range_iz = (mdn_model_iz.y_range[0],\n",
    "                mdn_model_iz.y_range[1])\n",
    "model_n_grid_iz = mdn_model_iz.n_grid\n",
    "y_grid_iz = np.linspace(model_y_range_iz[0], model_y_range_iz[1],\n",
    "                       model_n_grid_iz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:14:47.723961Z",
     "start_time": "2021-06-16T03:14:23.431025Z"
    }
   },
   "outputs": [],
   "source": [
    "y_delta_iz = (mdn_model_iz.y_range[1] - mdn_model_iz.y_range[0]) /\\\n",
    "            mdn_model_iz.n_grid\n",
    "\n",
    "cde_train_iz = mdn_model_iz.cde_predict_grid(x_train_iz).detach().numpy() \n",
    "cde_cal_iz = mdn_model_iz.cde_predict_grid(x_cal_iz).detach().numpy()\n",
    "cde_test_iz = mdn_model_iz.cde_predict_grid(x_test_iz).detach().numpy()\n",
    "\n",
    "t_grid = np.linspace(cde_train_iz.min(), cde_train_iz.max(), t_range_size)\n",
    "\n",
    "\n",
    "profile_density_train_iz = lc.profile_density(cde_train_iz, \n",
    "                                           t_grid = t_grid,\n",
    "                                           z_delta = y_delta_iz)\n",
    "\n",
    "profile_density_cal_iz = lc.profile_density(cde_cal_iz,\n",
    "                                         t_grid = t_grid,\n",
    "                                         z_delta = y_delta_iz)\n",
    "\n",
    "profile_density_test_iz = lc.profile_density(cde_test_iz,\n",
    "                                         t_grid = t_grid,\n",
    "                                         z_delta = y_delta_iz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:14:47.728935Z",
     "start_time": "2021-06-16T03:14:47.725411Z"
    }
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "quantiles = np.arange(1,500)/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:14:47.871890Z",
     "start_time": "2021-06-16T03:14:47.734038Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans_model_iz, _, _ = lc.profile_grouping(profile_density_train_iz, \n",
    "                                            k = k)\n",
    "\n",
    "_, _, grouping_cal_iz = lc.profile_grouping(profile_test = profile_density_cal_iz, Kmeans_model = kmeans_model_iz)\n",
    "_, _, grouping_test_iz= lc.profile_grouping(profile_test = profile_density_test_iz, Kmeans_model = kmeans_model_iz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:14:48.303224Z",
     "start_time": "2021-06-16T03:14:47.873860Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# get distribution of conformal scores -------\n",
    "cs_cal_iz = mdn_model_iz.cde_predict(x_cal_iz, y_cal_iz)\n",
    "\n",
    "df_cs_group_cal_iz = pd.DataFrame(data = {\"x\": x_cal_iz.numpy().ravel(),\n",
    "                                            \"cs\": cs_cal_iz.detach().numpy(),\n",
    "                                            \"grouping\":grouping_cal_iz})\n",
    "\n",
    "\n",
    "# will need to calculate cuts per each threshold per group\n",
    "\n",
    "cs_test_iz = mdn_model_iz.cde_predict(x_test_iz, y_test_iz)\n",
    "\n",
    "\n",
    "df_cs_group_test_iz = pd.DataFrame(data = {\"x\": x_test_iz.numpy().ravel(),\n",
    "                                           \"cs\": cs_test_iz.detach().numpy(),\n",
    "                                           \"grouping\":grouping_test_iz})\n",
    "\n",
    "thresholds_mat_test_iz, _ = lc.thresholds_per_group(df_cs_group_cal_iz,\n",
    "                                              desired_props = quantiles,\n",
    "                                              append = df_cs_group_test_iz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:17:15.656972Z",
     "start_time": "2021-06-16T03:14:48.306586Z"
    }
   },
   "outputs": [],
   "source": [
    "true_cde_test_iz = lc.true_cde_out(x_test_iz.numpy().ravel(), y_grid_iz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:17:21.611942Z",
     "start_time": "2021-06-16T03:17:15.658822Z"
    }
   },
   "outputs": [],
   "source": [
    "true_thresholds_test_iz = lc.true_thresholds_out(true_cde = true_cde_test_iz, \n",
    "                                                z_delta = y_delta_iz,\n",
    "                                                expected_prop = quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:17:21.632590Z",
     "start_time": "2021-06-16T03:17:21.613421Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def difference_validity_and_efficiency(true_cde, predict_grid,\n",
    "                                       true_grid,\n",
    "                                       thresholds_predict,\n",
    "                                       thresholds_true,\n",
    "                                       expected_prop,\n",
    "                                       z_delta = 1,\n",
    "                                       verbose = True):\n",
    "    \"\"\"\n",
    "    Calculates difference in validity and efficiency between predicted and true\n",
    "    estimates at an individual level.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    true_cde : numpy array (n, p). CDE estimates across a range of z values\n",
    "        that are separated by a common `z_delta`. Each row relates to a single\n",
    "        observation of a CDE function, and the columns of the array should\n",
    "        correspond to the CDE_i(y_j) evaluation.\n",
    "    predict_grid : numpy array (n, p). Predicted scores (e.g. HDP, CDE values,\n",
    "        etc.) across the same range of z values as `true_cde` with same\n",
    "        row/column structure as well\n",
    "    true_grid: numpy array (n, p). Same as `predict_grid` but the true scores\n",
    "        under the oracle (aka knowing `true_cde`)\n",
    "    thresholds_predict : numpy vector (n, m). Threshold cutoffs associated with the\n",
    "        predict_grid scores for each row of the above arrays (`true_cde`,\n",
    "        `predict_grid`, `true_grid`). The columns are associated with difference\n",
    "        cutoffs.\n",
    "    thresholds_true : numpy vector (n, m). Threshold cutoffs associated with the\n",
    "        true_grid scores for each row of the above arrays (`true_cde`,\n",
    "        `predict_grid`, `true_grid`). The columns are associated with difference\n",
    "        cutoffs.\n",
    "    expected_prop : numpy vector (m,). Amount of mass expected to be contained\n",
    "        in each level set defined by columns of `thresholds`\n",
    "    z_delta : difference between the range of z values used to create above\n",
    "        arrays (`true_cde`, `predict_grid`, `true_grid`)\n",
    "    verbose : boolean, logic if we should report progress of analysis\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    a tuple with the following info:\n",
    "        validity_error : numpy array (n, m). For each row (i) of the above numpy\n",
    "            matrices, we look at the absolute error between the expected_prop\n",
    "            verse actual mass in the level set. The columns are ranging across\n",
    "            the different thresholds and expected_prop values.\n",
    "        efficiency_error : numpy array (n, m). For each row (i) of the above\n",
    "            numpy matrices, we look at the lebegue measure of the set difference\n",
    "            between predicted level set and the true level set.\n",
    "\n",
    "    Details:\n",
    "    --------\n",
    "    This function can do both HDP and CDE style cutoffs (can actually do any\n",
    "    conformal score function evaluated on the same grid).\n",
    "\n",
    "    For **validity**: we calculate the discrete probability mass of the\n",
    "    predicted level set (at each given threshold) related to the expected_prop.\n",
    "\n",
    "    For **efficiency**: we calculate the lebegue set difference beween the\n",
    "    predicted level set vs the true level set (as defined with `true_grid`)\n",
    "    \"\"\"\n",
    "\n",
    "    # dimension checks -------\n",
    "\n",
    "    assert true_cde.shape == predict_grid.shape and \\\n",
    "        true_cde.shape == true_grid.shape, \\\n",
    "        \"dimensions of true_cde, predict_grid and true_grid should be the same\"\n",
    "\n",
    "    assert thresholds_predict.shape[1] == expected_prop.shape[0] and \\\n",
    "        thresholds_predict.shape == thresholds_true.shape, \\\n",
    "        \"number of columns of thresholds_{predict, true} should be the same \"+\\\n",
    "        \"as the length of expected_prop\"\n",
    "\n",
    "    assert thresholds_predict.shape[0] == true_cde.shape[0], \\\n",
    "        \"thresholds_{predict, true} should have the same number of rows as \"+\\\n",
    "        \"true_cde\"\n",
    "\n",
    "    # verbosity design (across thresholds) ---------\n",
    "    if verbose:\n",
    "        bar = progressbar.ProgressBar(widgets = [ progressbar.Bar(),\n",
    "                                              ' (', progressbar.ETA(), \", \",\n",
    "                                              progressbar.Timer(), ')'])\n",
    "        threshold_iter = bar(np.arange(thresholds_predict.shape[1]))\n",
    "    else:\n",
    "        threshold_iter = range(thresholds_predict.shape[1])\n",
    "\n",
    "\n",
    "    validity_error = -1 * np.ones((true_cde.shape[0],\n",
    "                                   thresholds_predict.shape[1]))\n",
    "    efficiency_error = -1 * np.ones((true_cde.shape[0],\n",
    "                                     thresholds_predict.shape[1]))\n",
    "\n",
    "    for t_idx in threshold_iter:\n",
    "        threshold_predict_vec = thresholds_predict[:,t_idx]\n",
    "\n",
    "        predict_indicator_ge_threshold = predict_grid >= \\\n",
    "            np.repeat(threshold_predict_vec.reshape((-1,1)),\n",
    "                      predict_grid.shape[1], axis = 1)\n",
    "\n",
    "\n",
    "        # oracle validity: with the underlying distribution ---------\n",
    "        expected_p = expected_prop[t_idx]\n",
    "\n",
    "        mask_cde_value = true_cde * predict_indicator_ge_threshold\n",
    "        rowwise_mass = mask_cde_value.sum(axis = 1) * z_delta\n",
    "\n",
    "        validity_error[:, t_idx] = np.abs(rowwise_mass - expected_p)\n",
    "\n",
    "\n",
    "        # efficiency: compared to truth optimal set ---------\n",
    "        threshold_true_vec = thresholds_true[:,t_idx]\n",
    "\n",
    "\n",
    "        true_indicator_ge_threshold = true_grid >= \\\n",
    "            np.repeat(threshold_true_vec.reshape((-1,1)),\n",
    "                      predict_grid.shape[1], axis = 1)\n",
    "\n",
    "        level_set_diff = true_indicator_ge_threshold == predict_indicator_ge_threshold\n",
    "        rowwise_diff = level_set_diff.sum(axis = 1)\n",
    "        efficiency_error[:, t_idx] = rowwise_diff * z_delta\n",
    "\n",
    "\n",
    "    return validity_error, efficiency_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:18:20.502052Z",
     "start_time": "2021-06-16T03:17:21.635054Z"
    }
   },
   "outputs": [],
   "source": [
    "v_mat_iz, e_mat_iz = lc.difference_validity_and_efficiency(true_cde = true_cde_test_iz,\n",
    "                                                 predict_grid = cde_test_iz,\n",
    "                                                 true_grid = true_cde_test_iz,\n",
    "                                                 thresholds_predict = thresholds_mat_test_iz,\n",
    "                                                 thresholds_true = true_tresholds_test_iz,\n",
    "                                                 expected_prop = quantiles, \n",
    "                                                 z_delta = y_delta_iz,\n",
    "                                                 verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:19:12.392570Z",
     "start_time": "2021-06-16T03:19:12.385682Z"
    }
   },
   "outputs": [],
   "source": [
    "test_bin_raw_iz = lc.difference_actual_validity(df_cs_group_test_iz,\n",
    "                                           thresholds_mat_test_iz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T17:52:28.247868Z",
     "start_time": "2021-06-14T17:52:28.219687Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def inner_process_izbicki(data_all,\n",
    "                         k_range, n_gaussians_range, lr_range, hidden1_range, \n",
    "                          hidden2_range, verbose = T, \n",
    "                          epochs = 10000, \n",
    "                          y_range = np.array([-25,25]).reshape((-1,1)),\n",
    "                          y_splits = 10000,\n",
    "                          quantiles = np.arange(1,500)/500,\n",
    "                          t_range_size = 500):\n",
    "    \n",
    "    data_izbicki_list = lc.stratified_data_splitting(data_all,\n",
    "                                                    prop_vec = np.array([1,1,0,1,1])/4)\n",
    "    x_train_iz,y_train_iz, g_train_iz = lc.torchify_data(data_izbicki_list[0].x, \n",
    "                                            data_izbicki_list[0].y, \n",
    "                                            data_izbicki_list[0].group_info)\n",
    "    x_val_iz,y_val_iz, g_val_iz = lc.torchify_data(data_izbicki_list[1].x, \n",
    "                                            data_izbicki_list[1].y, \n",
    "                                            data_izbicki_list[1].group_info)\n",
    "    x_cal_iz,y_cal_iz, g_val_iz = lc.torchify_data(data_izbicki_list[3].x, \n",
    "                                            data_izbicki_list[3].y, \n",
    "                                            data_izbicki_list[3].group_info)\n",
    "    x_test_iz,y_test_iz, g_test_iz = lc.torchify_data(data_izbicki_list[4].x, \n",
    "                                            data_izbicki_list[4].y, \n",
    "                                            data_izbicki_list[4].group_info)\n",
    "\n",
    "    \n",
    "    column_order = [\n",
    "                   \"approach_type\",\n",
    "                   \"mdn_n_gaussian\",\n",
    "                   \"mdn_lr\",\n",
    "                   \"mdn_hidden1\",\n",
    "                   \"mdn_hidden2\",\n",
    "                   \"mdn_train_error\",\n",
    "                   \"mdn_val_error\",\n",
    "                   \"q_lr\",\n",
    "                   \"q_hidden1\",\n",
    "                   \"q_hidden2\",\n",
    "                   \"q_train_error\",\n",
    "                   \"q_val_error\",\n",
    "                   \"k\",\n",
    "                   \"x_group\",\n",
    "                   \"validity_truth\",\n",
    "                   \"validity_test\",\n",
    "                   \"efficiency_truth\"\n",
    "                   ]\n",
    "    all_info_out = pd.DataFrame(columns = column_order)\n",
    "    \n",
    "    \n",
    "    for n_guassian, lr_1, hidden1_1, hidden2_1 in\n",
    "        itertools.product(n_gaussians_range, lr_range, hidden1_range, hidden2_range): # no k_range and only MDN parameters currently\t\t\t\n",
    "\n",
    "        # izbicki --------\n",
    "        mdn_model_iz, mdn_opt_iz, train_error_iz, val_error_iz = \\\n",
    "                                        lc.tune_first_nn(x_train = x_train_iz,\n",
    "                                                         y_train = y_train_iz,\n",
    "                                                         x_val = x_val_iz,\n",
    "                                                         y_val = v_val_iz,\n",
    "                                                         epochs = epochs,\n",
    "                                                         n_guassians=n_guassian,\n",
    "                                                         n_hidden1 = hidden1_1,\n",
    "                                                         n_hidden2 = hidden2_1,\n",
    "                                                         lr = lr_1)\n",
    "        mdn_model_iz.prep_for_cde(y_range, y_splits)\n",
    "        model_y_range_iz = (mdn_model_iz.y_range[0],\n",
    "                         mdn_model_iz.y_range[1])\n",
    "        model_n_grid_iz = mdn_model_iz.n_grid\n",
    "        y_grid_iz = np.linspace(model_y_range_iz[0], model_y_range_iz[1],\n",
    "                                model_n_grid_iz)\n",
    "\n",
    "        # izbicki clustering -----\n",
    "        y_delta_iz = (mdn_model_iz.y_range[1] - mdn_model_iz.y_range[0]) /\\\n",
    "                    mdn_model_iz.n_grid\n",
    "\n",
    "        cde_train_iz = mdn_model_iz.cde_predict_grid(x_train_iz).detach().numpy() \n",
    "        cde_cal_iz = mdn_model_iz.cde_predict_grid(x_cal_iz).detach().numpy()\n",
    "        cde_test_iz = mdn_model_iz.cde_predict_grid(x_test_iz).detach().numpy()\n",
    "        \n",
    "        t_grid = np.linspace(cde_mat.min(), cde_mat.max(), t_range_size)\n",
    "\n",
    "\n",
    "        profile_density_train_iz = lc.profile_density(cde_train_iz, \n",
    "                                                   t_grid = t_grid,\n",
    "                                                   z_delta = y_delta)\n",
    "\n",
    "        profile_density_cal_iz = lc.profile_density(cde_cal_iz,\n",
    "                                                 t_grid = t_grid,\n",
    "                                                 z_delta = y_delta)\n",
    "\n",
    "        profile_density_test_iz = lc.profile_density(cde_test_iz,\n",
    "                                                 t_grid = t_grid,\n",
    "                                                 z_delta = y_delta)\n",
    "        for k_val in k_range:\n",
    "\n",
    "            kmeans_model_iz, _, _ = lc.profile_grouping(profile_density_train_iz, \n",
    "                                                        k = k)\n",
    "            _, _, grouping_cal_iz = lc.profile_grouping(profile_test = profile_density_cal_iz, \n",
    "                                                        Kmeans_model = kmeans_model_iz)\n",
    "            _, _, grouping_test_iz = lc.profile_grouping(profile_test = profile_density_test_iz, \n",
    "                                                        Kmeans_model = kmeans_model_iz)\n",
    "\n",
    "\n",
    "\n",
    "            # get distribution of conformal scores -------\n",
    "            cs_cal_iz = mdn_model_iz.cde_predict(x_cal_iz, y_cal_iz)\n",
    "\n",
    "            df_cs_group_cal_iz = pd.DataFrame(data = {\"x\": x_cal_iz.numpy().ravel(),\n",
    "                                                      \"cs\": cs_cal_iz.detach().numpy(),\n",
    "                                                      \"grouping\":grouping_cal_iz})\n",
    "\n",
    "\n",
    "            # will need to calculate cuts per each threshold per group\n",
    "            cs_test_iz = mdn_model_iz.cde_predict(x_test_iz, y_test_iz)\n",
    "            \n",
    "            df_cs_group_test_iz = pd.DataFrame(data = {\"x\": x_test_iz.numpy().ravel(),\n",
    "                                                       \"cs\": cs_test_iz.detach().numpy(),\n",
    "                                                       \"grouping\":grouping_test_iz})\n",
    "\n",
    "            thresholds_mat_test_iz, _ = lc.thresholds_per_group(df_cs_group_cal_iz,\n",
    "                                                          desired_props = quantiles,\n",
    "                                                          append = df_cs_group_test_iz)\n",
    "\n",
    "\n",
    "            true_cde_test_iz = true_cde_out(x_test_iz.numpy().ravel(), y_grid_iz)\n",
    "            \n",
    "            true_thresholds_test_iz = lc.true_thresholds_out(true_cde = true_cde_test_iz, \n",
    "                                                z_delta = y_delta_iz,\n",
    "                                                expected_prop = quantiles)\n",
    "\n",
    "            v_mat_iz, e_mat_iz = lc.difference_validity_and_efficiency(true_cde = true_cde_test_iz,\n",
    "                                                             predict_grid = cde_test_iz,\n",
    "                                                             true_grid = true_cde_test_iz,\n",
    "                                                             thresholds_predict = thresholds_mat_test_iz,\n",
    "                                                             thresholds_true = true_tresholds_test_iz,\n",
    "                                                             expected_prop = quantiles, \n",
    "                                                             z_delta = y_delta_iz,\n",
    "                                                             verbose = False)\n",
    "            # write a function to evalulate test data proportions discretely\n",
    "            \n",
    "            v_test_bin_raw_iz = lc.difference_actual_validity(df_cs_group_test_iz,\n",
    "                                                              thresholds_mat_test_iz)\n",
    "            #^ this might be able to be put in diff_v_and_e...\n",
    "            # COME HERE!\n",
    "            #^ this will need to be transformed differently than v_mat_iz and e_mat_iz...\n",
    "            #^ we'll still need to process relative to desired outcome...\n",
    "            \n",
    "            ## g_test_iz (should this function actually be called in difference_.. functions? (yes))\n",
    "            _, v_average_iz = lc.average_within_groups(g_test_iz, v_mat_iz)\n",
    "            _, e_average_iz = lc.average_within_groups(g_test_iz, e_mat_iz)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #^still need to be averaged afterwards...\n",
    "            # average across true groups\n",
    "            # also do we melt that information into a df to be stored? (yes)\n",
    "            # ^ see average_within_groups for pd.concat approach... (hopefully)\n",
    "            \n",
    "\n",
    "            # how to combine items across ... \n",
    "            inner_info = pd.DataFrame({\"mdn_n_gaussian\": \"iz\",\n",
    "                                       \"mdn_lr\": lr_1,\n",
    "                                       \"mdn_hidden1\": hidden1_1,\n",
    "                                       \"mdn_hidden2\": hidden2_1,\n",
    "                                       \"mdn_train_error\": train_error_iz,\n",
    "                                       \"mdn_val_error\": val_error_iz,\n",
    "                                       \"q_lr\": None,\n",
    "                                       \"q_hidden1\": None,\n",
    "                                       \"q_hidden2\": None,\n",
    "                                       \"q_train_error\": None,\n",
    "                                       \"q_val_error\": None,\n",
    "                                       \"k\": k_val,\n",
    "                                       \"x_group\": ...vec,\n",
    "                                       \"validity_truth\": ...vec,\n",
    "                                       \"validity_test\": ...vec,\n",
    "                                       \"efficiency_truth\": ...vec},\n",
    "                                      columns = column_order)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "def inner_process_us(data_all, k_range, n_gaussians_range, lr_range, \n",
    "                     hidden1_range, hidden2_range, verbose = T, \n",
    "                     epochs = 10000, \n",
    "                     y_range = np.array([-25,25]).reshape((-1,1)),\n",
    "                     y_splits = 10000,\n",
    "                     quantiles = np.arange(1,500)/500, #we are using these quantiles relative to conformal cuts... that isn't what was expected\n",
    "                     t_range_size = 500):\n",
    "\n",
    "    \n",
    "    data_us_list = lc.stratified_data_splitting(data_all, \n",
    "                                                prop_vec = np.array([.75,.75,.75,.75,1])/4)\n",
    "    \n",
    "    x_train_us,y_train_us, g_train_us = lc.torchify_data(data_us_list[0].x, \n",
    "                                        data_us_list[0].y, \n",
    "                                        data_us_list[0].group_info)\n",
    "    x_val_us,y_val_us, g_val_us = lc.torchify_data(data_us_list[1].x, \n",
    "                                        data_us_list[1].y, \n",
    "                                        data_us_list[1].group_info)\n",
    "    x_cde_cal_us, y_cde_cal_us, lc.torchify_data(g_cde_cal_us = data_us_list[2].x, \n",
    "                                        data_us_list[2].y, \n",
    "                                        data_us_list[2].group_info)\n",
    "    x_cal_us,y_cal_us, g_cal_us = lc.torchify_data(data_us_list[3].x, \n",
    "                                        data_us_list[3].y, \n",
    "                                        data_us_list[3].group_info)\n",
    "    x_test_us,y_test_us, g_test_us = lc.torchify_data(data_us_list[4].x, \n",
    "                                        data_us_list[4].y, \n",
    "                                        data_us_list[4].group_info)\n",
    "    \n",
    "    column_order = [\n",
    "                   \"approach_type\",\n",
    "                   \"mdn_n_gaussian\",\n",
    "                   \"mdn_lr\",\n",
    "                   \"mdn_hidden1\",\n",
    "                   \"mdn_hidden2\",\n",
    "                   \"mdn_train_error\",\n",
    "                   \"mdn_val_error\",\n",
    "                   \"q_lr\",\n",
    "                   \"q_hidden1\",\n",
    "                   \"q_hidden2\",\n",
    "                   \"q_train_error\",\n",
    "                   \"q_val_error\",\n",
    "                   \"k\",\n",
    "                   \"x_group\",\n",
    "                   \"validity_truth\",\n",
    "                   \"validity_test\",\n",
    "                   \"efficiency_truth\"\n",
    "                   ]\n",
    "    \n",
    "    all_info_out = pd.DataFrame(columns = column_order)\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    for n_guassian, lr_1, hidden1_1, hidden2_1 in\n",
    "        itertools.product(n_gaussians_range, lr_range, \n",
    "                          hidden1_range, hidden2_range): # no k_range and only MDN parameters currently\n",
    "\n",
    "        # build MDN model (record training and validation loss) ------\n",
    "        \n",
    "        # us ---------\n",
    "        mdn_model_us, mdn_opt_us, train_error_us, val_error_us = lc.tune_first_nn(\n",
    "                                                         x_train = x_train_us,\n",
    "                                                         y_train = y_train_us,\n",
    "                                                         x_val = x_val_us,\n",
    "                                                         y_val = v_val_us,\n",
    "                                                         epochs = epochs,\n",
    "                                                         n_guassians=n_guassian,\n",
    "                                                         n_hidden1 = hidden1_1,\n",
    "                                                         n_hidden2 = hidden2_1,\n",
    "                                                         lr = lr_1)\n",
    "        mdn_model_us.prep_for_cde(y_range, y_splits)\n",
    "        model_y_range_us = (mdn_model_us.y_range[0],\n",
    "                         mdn_model_us.y_range[1])\n",
    "        model_n_grid_us = mdn_model_us.n_grid\n",
    "        y_grid_us = np.linspace(model_y_range_us[0], model_y_range_us[1], model_n_grid_us)\n",
    "        \n",
    "        # prepping for second model:\n",
    "        cde_train_us = mdn_model_us.cde_predict_grid(x_train_us).detach().numpy()\n",
    "        cde_val_us = mdn_model_us.cde_predict_grid(x_val_us).detach().numpy()\n",
    "        \n",
    "        hpd_train_us = lc.hpd_coverage(cde_train_us, y_grid_us, y_train_us.numpy())\n",
    "        hpd_val_us = lc.hpd_coverage(cde_val_us, y_grid_us, y_val_us.numpy())\n",
    "        \n",
    "        \n",
    "        # TODO: tune_first_nn & tune_second_nn: record training error too!\n",
    "        \n",
    "        for lr_2, hidden1_1, hidden2_1 in itertools.product(lr_range, hidden1_range, hidden2_range): \n",
    "            # build Quantile Reg model (record training and validation loss) ---\n",
    "            \n",
    "            q_model, q_opt, q_train_error_us, q_val_error_us = lc.tune_second_nn(x_train = x_train_us,\n",
    "                                                            y_train = hpd_train_us,\n",
    "                                                            x_val = x_val_us,\n",
    "                                                            y_val = hpd_val_us,\n",
    "                                                            epochs = epochs,\n",
    "                                                            quantiles = quantiles,\n",
    "                                                            n_hidden1 = hidden1_2,\n",
    "                                                            n_hidden2 = hidden2_2,\n",
    "                                                            lr = lr_2,\n",
    "                                                            verbose = False)\n",
    "            \n",
    "            hpd_estimate_cal_us = q_model(x_cal_us).detach().numpy()\n",
    "            hpd_estimate_test_us = q_model(x_test_us).detach().numpy()\n",
    "            \n",
    "            # apply groupings  -----\n",
    "            for k_val in k_range:\n",
    "                kmeans_model_us, grouping_cal_us, _ = \\\n",
    "                    lc.profile_grouping(hpd_estimate_cal_us, k = k)\n",
    "\n",
    "                _, grouping_test_iz, _ = lc.profile_grouping(profile_density_test_iz, ...kmeans_model_us)\n",
    "            \n",
    "                \n",
    "                # need to convert CDE to HDP for our approach...\n",
    "                \n",
    "                df_cs_group_cal_us = pd.DataFrame(data = {\"x\": x_cal_us.numpy().ravel(),\n",
    "                                                        \"cs\": ...,\n",
    "                                                        \"grouping\":grouping_cal_us})\n",
    "            \n",
    "\n",
    "\n",
    "                df_cs_group_test_us = pd.DataFrame(data = {\"x\": x_test_us.numpy().ravel(),\n",
    "                                                           \"cs\": ...,\n",
    "                                                           \"grouping\":grouping_test_us})\n",
    "            \n",
    "                thresholds_mat_test_us, _ = thresholds_per_group(df_cs_group_cal_us,\n",
    "                                                              desired_props = quantiles,\n",
    "                                                              append = df_cs_group_test_us)\n",
    "\n",
    "                v_mat_us, e_mat_us = diference_validity_and_efficiency(true_cde = ...true_cde_test_us,\n",
    "                                                     predict_grid = ...hpd_grid_test_us,\n",
    "                                                     true_grid = ...true_hpd_test_us,\n",
    "                                                     thresholds_predict = ...thresholds_hpd_test_us,\n",
    "                                                     thresholds_grid = ...true_hdp_test_us,\n",
    "                                                     expected_prop = quantiles, \n",
    "                                                     z_delta = y_delta_us,\n",
    "                                                     verbose = False)\n",
    "            \n",
    "                inner_info = pd.DataFrame({\"mdn_n_gaussian\": \"us\",\n",
    "                                       \"mdn_lr\": lr_1,\n",
    "                                       \"mdn_hidden1\": hidden1_1,\n",
    "                                       \"mdn_hidden2\": hidden2_1,\n",
    "                                       \"mdn_train_error\": train_error_us,\n",
    "                                       \"mdn_val_error\": val_error_us,\n",
    "                                       \"q_lr\": lr_2,\n",
    "                                       \"q_hidden1\": hidden1_2,\n",
    "                                       \"q_hidden2\": hidden2_1,\n",
    "                                       \"q_train_error\": q_train_error_us,\n",
    "                                       \"q_val_error\": q_val_error_us,\n",
    "                                       \"k\": k_val,\n",
    "                                       \"x_group\": ...vec,\n",
    "                                       \"validity_truth\": ...vec,\n",
    "                                       \"validity_test\": ...vec,\n",
    "                                       \"efficiency_truth\": ...vec},\n",
    "                                          columns = column_order)\n",
    "                \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "227px",
    "width": "188px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
